fix_dupes

A simple script I wrote that walks through a hierarchy of directories, and
looks for duplicate files.  If it finds any, it will hard link them to each
other.

This was a result of me trying to organise my 'media' folders.  I had movies,
mp3s, and tvshows that I'd ripped from disc and then copied around into
hierarchies, but then realised that I had copies in different locations of the
same file for different media serving devices.  This was a waste of space.

It will simply go through a hierarchy, and md5sum each file.  If their md5sum
matches, and they aren't already pointing at the same inode, it will rm one of
the copies, and then cp -l them.

It does most of its work via shell, and is pretty rudimentary, but it saved me
a bunch of space.

NO WARRANTY

Please keep in mind, this is a simple one-off -- if you want to be sure it's
going to do the right thing, then comment out the unlink and cp -l below, and
run it so it just outputs what it WOULD do.

Even then, it could blow away data if it screws up.  Keep that in mind.

Also keep in mind there's the possibility for md5 hash collisions, as well as
the fact that if a file is > 100M in size, it will only md5sum the first and
last MB of the file.  This is just for time savings.  You could comment that
out too if you so desired.

If I find more than just me is using this, I might update it to support
--dry-run or to only output shell commands you can peruse and run yourself or
something.

But either way: use this program at your own risk.  Please don't get mad at me
if it eats your data or sets your cat on fire.
